{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8e66b4",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8970dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import datasets\n",
    "import numpy as np\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()\n",
    "\n",
    "#mnist = fetch_openml(\"mnist_784\")\n",
    "\n",
    "#X, y = fetch_openml(\"mnist_784\", return_X_y=True)\n",
    "\n",
    "#X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False, parser=\"pandas\")\n",
    "\n",
    "#X = X / 255\n",
    "\n",
    "#X = mnist[\"data\"]\n",
    "#y = mnist[\"target\"]\n",
    "\n",
    "#print(X[:5])\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "data = datasets.mnist.load_data()\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = data\n",
    "\n",
    "#train_data = np.reshape(train_data,[784,42000])\n",
    "#train_label = np.zeros((10,42000))\n",
    "\n",
    "#print(datasets.mnist.load_data())\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "X_train = np.reshape(X_train, [60000, 28 * 28])\n",
    "X_test = np.reshape(X_test, [10000, 28 * 28])\n",
    "\n",
    "print(X_train[0])\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "image_size = 28 * 28 # image size is 28x28 pixels\n",
    "hidden_layer_size = 100 # chosen size of hidden layer; can be any number\n",
    "output_layer_size = 10 # becuase of digits from 0 to 9\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "331c57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Layer_ReLU:\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward_pass(self, x):\n",
    "        \n",
    "        return (x > 0) * 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06804dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Layer_Sigmoid:\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward_pass(self, x):\n",
    "        \n",
    "        return self.forward_pass(x) * (1 - self.forward_pass(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e0c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Layer_Softmax:\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        exponents = np.exp(x - np.max(x, axis = 1, keepdims = True))\n",
    "        \n",
    "        return (exponents) / np.sum(exponents)\n",
    "    \n",
    "    def backward_pass(self, x):\n",
    "        \n",
    "        softmax = self.softmax(x)\n",
    "        \n",
    "        return np.diag(softmax) + (softmax * softmax.reshape(14000, 784))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd6f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        return np.dot(x, self.weights) + self.biases\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880f1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def calculate_mean_loss(self, y_pred, y_true):\n",
    "        \n",
    "        sample_loss = self.forward_propogation(y_pred, y_true)\n",
    "        \n",
    "        mean_loss = np.mean(sample_loss)\n",
    "        \n",
    "        return mean_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecb98b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Cross_Entropy(Loss):\n",
    "    \n",
    "    def forward_propogation(self, y_pred, y_true):\n",
    "        \n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            \n",
    "            #print(\"A:\", y_pred.shape, y_true.shape, range(samples))\n",
    "            \n",
    "            confidence = y_pred[range(samples), y_true]\n",
    "        \n",
    "        elif len(y_true.shape) == 2:\n",
    "            \n",
    "            confidence = np.sum(y_pred * y_true, axis = 1)\n",
    "            \n",
    "        neg_log_likelihood = -np.log(confidence)\n",
    "        \n",
    "        return neg_log_likelihood\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda2751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer1 = Layer_Dense(X_train.shape[1], epochs)\n",
    "\n",
    "activation_layer1 = Activation_Layer_ReLU()\n",
    "\n",
    "dense_layer2 = Layer_Dense(epochs, epochs)\n",
    "\n",
    "activation_layer2 = Activation_Layer_Sigmoid()\n",
    "\n",
    "dense_layer3 = Layer_Dense(epochs, epochs)\n",
    "\n",
    "activation_layer3 = Activation_Layer_Softmax()\n",
    "\n",
    "loss_function = Loss_Cross_Entropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4606b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_loss = 9999999\n",
    "\n",
    "best_dense_layer1_weights = dense_layer1.weights.copy()\n",
    "best_dense_layer1_biases = dense_layer1.biases.copy()\n",
    "\n",
    "best_dense_layer2_weights = dense_layer2.weights.copy()\n",
    "best_dense_layer2_biases = dense_layer2.biases.copy()\n",
    "\n",
    "best_dense_layer3_weights = dense_layer3.weights.copy()\n",
    "best_dense_layer3_biases = dense_layer3.biases.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f93ce39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: 13.307504760643193 acc: 0.10218333333333333\n",
      "iteration: 1000 loss: 12.780883493017255 acc: 0.4464666666666667\n",
      "iteration: 2000 loss: 12.523787373937408 acc: 0.5032\n",
      "iteration: 3000 loss: 12.362424288289546 acc: 0.57075\n",
      "iteration: 4000 loss: 12.20588202737463 acc: 0.6178833333333333\n",
      "iteration: 5000 loss: 12.144667362049454 acc: 0.6494666666666666\n",
      "iteration: 6000 loss: 12.112018868449486 acc: 0.66285\n",
      "iteration: 7000 loss: 12.103830907982243 acc: 0.6597833333333334\n",
      "iteration: 8000 loss: 12.064912754829617 acc: 0.66475\n",
      "iteration: 9000 loss: 12.055176015733013 acc: 0.6756833333333333\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10000):\n",
    "\n",
    "    dense_layer1.weights += 0.05 * np.random.randn(X_train.shape[1], epochs)\n",
    "    dense_layer1.biases += 0.05 * np.random.randn(1, epochs)\n",
    "    \n",
    "    dense_layer2.weights += 0.05 * np.random.randn(epochs, epochs)\n",
    "    dense_layer2.biases += 0.05 * np.random.randn(1, epochs)\n",
    "    \n",
    "    dense_layer3.weights += 0.05 * np.random.randn(epochs, epochs)\n",
    "    dense_layer3.biases += 0.05 * np.random.randn(1, epochs)\n",
    "    \n",
    "    dense_layer1_forward = dense_layer1.forward_pass(X_train)\n",
    "    \n",
    "    #print(X_train.shape[1])\n",
    "    #print(dense_layer1_forward.shape)\n",
    "    \n",
    "    activation_layer1_forward = activation_layer1.forward_pass(dense_layer1_forward)\n",
    "\n",
    "    dense_layer2_forward = dense_layer2.forward_pass(activation_layer1_forward)\n",
    "\n",
    "    activation_layer2_forward = activation_layer2.forward_pass(dense_layer2_forward)\n",
    "\n",
    "    dense_layer3_forward = dense_layer3.forward_pass(activation_layer2_forward)\n",
    "    \n",
    "    activation_layer3_forward = activation_layer3.forward_pass(dense_layer3_forward)\n",
    "    \n",
    "    #print(y_test.shape)\n",
    "    \n",
    "    #print(activation_layer3_forward)\n",
    "    \n",
    "    loss = loss_function.calculate_mean_loss(activation_layer3_forward, y_train)\n",
    "    \n",
    "    #print(\"Loss:\", loss)\n",
    "    \n",
    "    predictions = np.argmax(activation_layer3_forward, axis = 1)\n",
    "\n",
    "    accuracy = np.mean(predictions == y_train)\n",
    "\n",
    "    #print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    if loss < lowest_loss:\n",
    "        \n",
    "        #print('New set of weights found, iteration:', iteration, 'loss:', loss, 'acc:', accuracy)\n",
    "        \n",
    "        best_dense_layer1_weights = dense_layer1.weights.copy()\n",
    "        best_dense_layer1_biases = dense_layer1.biases.copy()\n",
    "        \n",
    "        best_dense_layer2_weights = dense_layer2.weights.copy()\n",
    "        best_dense_layer2_biases = dense_layer2.biases.copy()\n",
    "        \n",
    "        best_dense_layer3_weights = dense_layer3.weights.copy()\n",
    "        best_dense_layer3_biases = dense_layer3.biases.copy()\n",
    "        \n",
    "        lowest_loss = loss\n",
    "        \n",
    "    # Revert weights and biases\n",
    "    else:\n",
    "        dense_layer1.weights = best_dense_layer1_weights.copy()\n",
    "        dense_layer1.biases = best_dense_layer1_biases.copy()\n",
    "        \n",
    "        dense_layer2.weights = best_dense_layer2_weights.copy()\n",
    "        dense_layer2.biases = best_dense_layer2_biases.copy()\n",
    "        \n",
    "        dense_layer3.weights = best_dense_layer3_weights.copy()\n",
    "        dense_layer3.biases = best_dense_layer3_biases.copy()\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "        \n",
    "        print(\"iteration:\", iteration, \"loss:\", loss, \"acc:\", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51448581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 12.033241483160662\n",
      "Accuracy: 0.6847\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987a440",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5d1b248c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.67590275e-06 1.65152065e-06 1.65721082e-06 ... 1.65715403e-06\n",
      "  1.67848615e-06 1.67018360e-06]\n",
      " [1.67588782e-06 1.65155903e-06 1.65721330e-06 ... 1.65719159e-06\n",
      "  1.67850166e-06 1.67018850e-06]\n",
      " [1.67591281e-06 1.65160950e-06 1.65722753e-06 ... 1.65719984e-06\n",
      "  1.67855277e-06 1.67021450e-06]\n",
      " ...\n",
      " [1.67591354e-06 1.65152998e-06 1.65723682e-06 ... 1.65715801e-06\n",
      "  1.67850356e-06 1.67018524e-06]\n",
      " [1.67590307e-06 1.65158943e-06 1.65719749e-06 ... 1.65718721e-06\n",
      "  1.67852821e-06 1.67022410e-06]\n",
      " [1.67589272e-06 1.65154629e-06 1.65720792e-06 ... 1.65716691e-06\n",
      "  1.67847822e-06 1.67018461e-06]]\n",
      "Loss: 13.304818431623891\n"
     ]
    }
   ],
   "source": [
    "dense_layer1_forward = dense_layer1.forward_pass(X_train)\n",
    "\n",
    "#print(X_train.shape[1])\n",
    "#print(dense_layer1_forward.shape)\n",
    "\n",
    "activation_layer1_forward = activation_layer1.forward_pass(dense_layer1_forward)\n",
    "\n",
    "dense_layer2_forward = dense_layer2.forward_pass(activation_layer1_forward)\n",
    "\n",
    "activation_layer2_forward = activation_layer2.forward_pass(dense_layer2_forward)\n",
    "\n",
    "dense_layer3_forward = dense_layer3.forward_pass(activation_layer2_forward)\n",
    "\n",
    "activation_layer3_forward = activation_layer3.forward_pass(dense_layer3_forward)\n",
    "\n",
    "#print(y_test.shape)\n",
    "\n",
    "print(activation_layer3_forward)\n",
    "\n",
    "loss = loss_function.calculate_mean_loss(activation_layer3_forward, y_train)\n",
    "\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "03c35d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10218333333333333\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(activation_layer3_forward, axis = 1)\n",
    "\n",
    "accuracy = np.mean(predictions == y_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3cd42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56617216",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbc9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, no_of_in_nodes, no_of_out_nodes, no_of_hidden_nodes, learning_rate):\n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes\n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes\n",
    "        self.learning_rate = learning_rate \n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e88fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a881e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    \n",
    "    return np.maximum(0, x)\n",
    "\n",
    "w1 = rng.random((image_size, hidden_layer_size))\n",
    "w2 = rng.random((hidden_layer_size, output_layer_size))\n",
    "\n",
    "print(w1.shape)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    \n",
    "    for j in range(len(X_train)):\n",
    "        \n",
    "        layer_0 = X_train[j]\n",
    "        \n",
    "        layer_1 = np.dot(layer_0, w1)\n",
    "        layer_1 = relu(layer_1)\n",
    "        \n",
    "        layer_2 = np.dot(layer_1, w2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08320e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
