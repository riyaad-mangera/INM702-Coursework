{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8e66b4",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8970dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "[7 2 1 ... 4 5 6]\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "from keras import datasets\n",
    "import numpy as np\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = default_rng()\n",
    "encoder = OneHotEncoder()\n",
    "encoder2= OneHotEncoder()\n",
    "\n",
    "data = datasets.mnist.load_data()\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = data\n",
    "\n",
    "#print(datasets.mnist.load_data())\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "X_train = np.reshape(X_train, [60000, 28 * 28])\n",
    "X_test = np.reshape(X_test, [10000, 28 * 28])\n",
    "\n",
    "encoder = encoder.fit_transform(y_train.reshape(1, -1))\n",
    "encoder2= encoder2.fit_transform(y_test.reshape(1, -1))\n",
    "#y_train = np.reshape(y_train, [60000, 1])\n",
    "#y_test = np.reshape(y_test, [10000, 1])\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_test)\n",
    "\n",
    "learning_rate = 0.05\n",
    "epochs = 10000\n",
    "image_size = 28 * 28 # image size is 28x28 pixels\n",
    "input_layer_size = X_train.shape[1]\n",
    "hidden_layer_size = 10 # chosen size of hidden layer; can be any number\n",
    "output_layer_size = 10 # becuase of digits from 0 to 9\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331c57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Layer_ReLU:\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward_pass(self, x):\n",
    "        \n",
    "        return (x > 0) * 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06804dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Layer_Sigmoid:\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward_pass(self, x):\n",
    "        \n",
    "        return self.forward_pass(x) * (1 - self.forward_pass(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27e0c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Layer_Softmax:\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        exponents = np.exp(x - np.max(x, axis = 1, keepdims = True))\n",
    "        \n",
    "        return (exponents) / np.sum(exponents)\n",
    "    \n",
    "    def backward_pass(self, x):\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdd6f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        return np.dot(x, self.weights) + self.biases\n",
    "    \n",
    "    def backward_pass(self, x):\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880f1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def calculate_mean_loss(self, y_pred, y_true):\n",
    "        \n",
    "        sample_loss = self.forward_propogation(y_pred, y_true)\n",
    "        \n",
    "        mean_loss = np.mean(sample_loss)\n",
    "        \n",
    "        return mean_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb98b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Cross_Entropy(Loss):\n",
    "    \n",
    "    def forward_propogation(self, y_pred, y_true):\n",
    "        \n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        confidence = y_pred[range(samples), y_true]\n",
    "        \n",
    "        neg_log_likelihood = -np.log(confidence)\n",
    "        \n",
    "        return neg_log_likelihood\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda2751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer1 = Layer_Dense(X_train.shape[1], hidden_layer_size)\n",
    "\n",
    "activation_layer1 = Activation_Layer_ReLU()\n",
    "\n",
    "dense_layer2 = Layer_Dense(hidden_layer_size, hidden_layer_size)\n",
    "\n",
    "activation_layer2 = Activation_Layer_Sigmoid()\n",
    "\n",
    "dense_layer3 = Layer_Dense(hidden_layer_size, output_layer_size)\n",
    "\n",
    "activation_layer3 = Activation_Layer_Softmax()\n",
    "\n",
    "loss_function = Loss_Cross_Entropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e97ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 13.304669751665982 Accuracy: 0.10441666666666667\n"
     ]
    }
   ],
   "source": [
    "dense_layer1_forward = dense_layer1.forward_pass(X_train)\n",
    "   \n",
    "#print(X_train.shape[1])\n",
    "#print(dense_layer1_forward.shape)\n",
    "\n",
    "activation_layer1_forward = activation_layer1.forward_pass(dense_layer1_forward)\n",
    "\n",
    "dense_layer2_forward = dense_layer2.forward_pass(activation_layer1_forward)\n",
    "\n",
    "activation_layer2_forward = activation_layer2.forward_pass(dense_layer2_forward)\n",
    "\n",
    "dense_layer3_forward = dense_layer3.forward_pass(activation_layer2_forward)\n",
    "\n",
    "activation_layer3_forward = activation_layer3.forward_pass(dense_layer3_forward)\n",
    "    \n",
    "#print(y_test.shape)\n",
    "\n",
    "#print(activation_layer3_forward)\n",
    "\n",
    "loss = loss_function.calculate_mean_loss(activation_layer3_forward, y_train)\n",
    "\n",
    "#print(\"Loss:\", loss)\n",
    "\n",
    "y_pred = np.argmax(activation_layer3_forward, axis = 1)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "\n",
    "print(\"Loss:\", loss, \"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1931ed76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.69000077e-06 1.64498450e-06 1.63841815e-06 ... 1.70788399e-06\n",
      "  1.67873613e-06 1.66499470e-06]\n",
      " [1.68996885e-06 1.64502080e-06 1.63843248e-06 ... 1.70788399e-06\n",
      "  1.67872795e-06 1.66498335e-06]\n",
      " [1.68996749e-06 1.64501049e-06 1.63841002e-06 ... 1.70788399e-06\n",
      "  1.67870744e-06 1.66498943e-06]\n",
      " ...\n",
      " [1.68993311e-06 1.64486594e-06 1.63836922e-06 ... 1.70788399e-06\n",
      "  1.67869176e-06 1.66494915e-06]\n",
      " [1.68998048e-06 1.64503490e-06 1.63843662e-06 ... 1.70788399e-06\n",
      "  1.67872951e-06 1.66499991e-06]\n",
      " [1.68998451e-06 1.64493789e-06 1.63839364e-06 ... 1.70788399e-06\n",
      "  1.67873083e-06 1.66497587e-06]]\n"
     ]
    }
   ],
   "source": [
    "print(activation_layer3_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c2aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bed11b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 13.304669751665982\n",
      "Accuracy: 0.10441666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b2bea",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5d1b248c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.67590275e-06 1.65152065e-06 1.65721082e-06 ... 1.65715403e-06\n",
      "  1.67848615e-06 1.67018360e-06]\n",
      " [1.67588782e-06 1.65155903e-06 1.65721330e-06 ... 1.65719159e-06\n",
      "  1.67850166e-06 1.67018850e-06]\n",
      " [1.67591281e-06 1.65160950e-06 1.65722753e-06 ... 1.65719984e-06\n",
      "  1.67855277e-06 1.67021450e-06]\n",
      " ...\n",
      " [1.67591354e-06 1.65152998e-06 1.65723682e-06 ... 1.65715801e-06\n",
      "  1.67850356e-06 1.67018524e-06]\n",
      " [1.67590307e-06 1.65158943e-06 1.65719749e-06 ... 1.65718721e-06\n",
      "  1.67852821e-06 1.67022410e-06]\n",
      " [1.67589272e-06 1.65154629e-06 1.65720792e-06 ... 1.65716691e-06\n",
      "  1.67847822e-06 1.67018461e-06]]\n",
      "Loss: 13.304818431623891\n"
     ]
    }
   ],
   "source": [
    "dense_layer1_forward = dense_layer1.forward_pass(X_train)\n",
    "\n",
    "#print(X_train.shape[1])\n",
    "#print(dense_layer1_forward.shape)\n",
    "\n",
    "activation_layer1_forward = activation_layer1.forward_pass(dense_layer1_forward)\n",
    "\n",
    "dense_layer2_forward = dense_layer2.forward_pass(activation_layer1_forward)\n",
    "\n",
    "activation_layer2_forward = activation_layer2.forward_pass(dense_layer2_forward)\n",
    "\n",
    "dense_layer3_forward = dense_layer3.forward_pass(activation_layer2_forward)\n",
    "\n",
    "activation_layer3_forward = activation_layer3.forward_pass(dense_layer3_forward)\n",
    "\n",
    "#print(y_test.shape)\n",
    "\n",
    "print(activation_layer3_forward)\n",
    "\n",
    "loss = loss_function.calculate_mean_loss(activation_layer3_forward, y_train)\n",
    "\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3d745354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10218333333333333\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(activation_layer3_forward, axis = 1)\n",
    "\n",
    "accuracy = np.mean(predictions == y_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6612e37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56617216",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbc9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, no_of_in_nodes, no_of_out_nodes, no_of_hidden_nodes, learning_rate):\n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes\n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes\n",
    "        self.learning_rate = learning_rate \n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e88fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a881e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    \n",
    "    return np.maximum(0, x)\n",
    "\n",
    "w1 = rng.random((image_size, hidden_layer_size))\n",
    "w2 = rng.random((hidden_layer_size, output_layer_size))\n",
    "\n",
    "print(w1.shape)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    loss = 0.0\n",
    "    \n",
    "    for j in range(len(X_train)):\n",
    "        \n",
    "        layer_0 = X_train[j]\n",
    "        \n",
    "        layer_1 = np.dot(layer_0, w1)\n",
    "        layer_1 = relu(layer_1)\n",
    "        \n",
    "        layer_2 = np.dot(layer_1, w2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08320e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
