{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2830186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from datetime import datetime\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301ef61",
   "metadata": {},
   "source": [
    "## Check whether to use GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ff2454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"Device:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569c206",
   "metadata": {},
   "source": [
    "## Loading train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20f067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 69, 69]) 0\n",
      "Classes: \n",
      " ['E', 'S', 'SB']\n"
     ]
    }
   ],
   "source": [
    "train_path = \"archive\\images_E_S_SB_69x69_a_03\\images_E_S_SB_69x69_a_03_train\"\n",
    "test_path = \"archive\\images_E_S_SB_69x69_a_03\\images_E_S_SB_69x69_a_03_test\"\n",
    "\n",
    "dataset = ImageFolder(train_path, transform = transforms.Compose([transforms.Resize((69,69)),transforms.ToTensor()]))\n",
    "\n",
    "test_dataset = ImageFolder(test_path, transforms.Compose([transforms.Resize((69,69)),transforms.ToTensor()]))\n",
    "\n",
    "img, label = dataset[0]\n",
    "\n",
    "print(img.shape,label)\n",
    "print(\"Classes: \\n\",dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea6b89d",
   "metadata": {},
   "source": [
    "## Display sample image to confirm data was loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d9a7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: E\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABFAEUDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDwLPPQU70OetNJBI+XGKfnYcgL0pAJkE5yeKXG4/KCAafGuWUk/ePQVKsOSxUnA55pNgkVxkvyO2KeiAMQecc5q6bMvIPm4YdqY0GzdGqt838VTzFOOhSx83TGTSsgTOeakCsXwvLDgg0EZIBGAetO5BCsRYZxiikJ2sRk47UVQCtkfKeFzT1WMDJBJBoXBLZ5qaFT5bMQNvvSbGLCmTuUkk9PatK3smyCfTnJp1nAJeVK+uBXT6dpe/ACkbvXvWM5pGkNTLjsWCcDBPTio57IKmGBJrvI9Ak8oOU4xWRqNl5QII6VzKupbGrjocHPAbfewjAJ71QZWUbjzmuhv0wxDdKw7jDTBQ2E6Zrrg7oxdkVCMnjP40U9yAcAbsd6K0I5hFXBzng9BVyFQ+EJwOh47VTjba4+Xt1NXIJHLFQEBHepkBu6bGpZTwB0Xiu90FYxJGWrhLGRQA24cDpXRWd+YsEHGK46seY6KSPWbi6sV04KuAwFeca1cI0jbcEE80yXWmaPBc/nXPX+oBycHpWNOjZnTKUWjM1STONy5HesKR0aRh0A561d1G8BxsBJ71lFt5yo+td8I2RxVLCMpDYDDA6UVGWViT0orUyFORuOO9SI67c4yRzUO4gc9KdlcDFFgNW3uW2qQQCei1ow6iU3Kw6GucLsApHCjpU4nkUDJyc9azlDU15tNDdn1MMuVHTrmqE18HBAwOOorPe6LMp6qOoqNnLLypUE8YqlFISkxxYu33zt9aiz82Qc+tIQRjOcUm/kgACrIuN4opQhPp+NFAhrMenpT0XK5zRRQMd5hAIwOKQEtgk9KKKGIRTuJBHWntlWVScj3oooH0GOcSYHSmscE0UUCJAgxmiiigZ//9k=\n",
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=69x69>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_img(img, label):\n",
    "    \n",
    "    print(\"Type:\", dataset.classes[label])\n",
    "    \n",
    "    to_img = transforms.ToPILImage()(img)\n",
    "    display(to_img)\n",
    "\n",
    "    \n",
    "display_img(*dataset[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b5d68",
   "metadata": {},
   "source": [
    "## Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8e108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 108389\n",
      "Validation Data: 12043\n"
     ]
    }
   ],
   "source": [
    "valid_size = int(len(dataset) * 0.1)\n",
    "train_size = len(dataset) - valid_size \n",
    "\n",
    "train_data, valid_data = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "print(\"Train Data:\", len(train_data))\n",
    "print(\"Validation Data:\",  len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785864de",
   "metadata": {},
   "source": [
    "## Generate batches for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af906f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_batches = DataLoader(train_data, batch_size, shuffle = True, num_workers = 2, pin_memory = True)\n",
    "valid_batches = DataLoader(valid_data, batch_size * 2, num_workers = 2, pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e42d3b",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceca1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.1, epochs = 5, optimiser = torch.optim.SGD):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.optimiser = optimiser\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(16384, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 12)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.network(x)\n",
    "    \n",
    "    def accuracy(self, outputs, labels):\n",
    "        \n",
    "        _, predictions = torch.max(outputs, dim = 1)\n",
    "        \n",
    "        return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "\n",
    "    @torch.no_grad() #Updates weights without calculating gradients\n",
    "    def evaluate(self, model, valid_batches):\n",
    "        \n",
    "        model.eval()\n",
    "        outputs = []\n",
    "        \n",
    "        for batch in valid_batches:\n",
    "            \n",
    "            images, labels = batch \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            accuracy = self.accuracy(predictions, labels)\n",
    "            \n",
    "            outputs.append({\"valid_loss\": loss.detach(), \"valid_accuracy\": accuracy})\n",
    "    \n",
    "        #Combine losses and accuracies\n",
    "        batch_loss = [x[\"valid_loss\"] for x in outputs]\n",
    "        batch_accuracy = [x[\"valid_accuracy\"] for x in outputs]\n",
    "        \n",
    "        epoch_loss = torch.stack(batch_loss).mean()\n",
    "        epoch_accuracy = torch.stack(batch_accuracy).mean()\n",
    "        \n",
    "        return {\"valid_loss\": epoch_loss.item(), \"valid_accuracy\": epoch_accuracy.item()}\n",
    "    \n",
    "    def fit(self, model, train_batches, valid_batches):\n",
    "    \n",
    "        history = []\n",
    "        optimizer = self.optimiser(model.parameters(), self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            start = datetime.now()\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "\n",
    "            for batch in train_batches:\n",
    "\n",
    "                images, labels = batch \n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                predictions = model(images)\n",
    "                loss = F.cross_entropy(predictions, labels)\n",
    "                \n",
    "                train_losses.append(loss)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            result = self.evaluate(model, valid_batches)\n",
    "            result[\"train_loss\"] = torch.stack(train_losses).mean().item()\n",
    "            \n",
    "            print(\"Epoch [{}], train_loss: {:.4f}, valid_loss: {:.4f}, valid_accuracy: {:.4f}\".format(\n",
    "                epoch, result[\"train_loss\"], result[\"valid_loss\"], result[\"valid_accuracy\"]))\n",
    "            print(f\"Time taken for epoch {epoch}: {datetime.now()-start}\")\n",
    "            \n",
    "            history.append(result)\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2eef68",
   "metadata": {},
   "source": [
    "## Initialise model and set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50da34bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNeuralNetwork(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Flatten(start_dim=1, end_dim=-1)\n",
      "    (16): Linear(in_features=16384, out_features=1024, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=512, out_features=12, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "optimiser = torch.optim.Adam\n",
    "\n",
    "model = ImageNeuralNetwork(learning_rate, epochs, optimiser)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0ff97",
   "metadata": {},
   "source": [
    "## Fit model and get recorded loss values of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27d6b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 0.6537, valid_loss: 0.5294, valid_accuracy: 0.7948\n",
      "Time taken for epoch 0: 0:02:25.024704\n",
      "Epoch [1], train_loss: 0.4932, valid_loss: 0.4727, valid_accuracy: 0.8217\n",
      "Time taken for epoch 1: 0:02:21.505364\n",
      "Epoch [2], train_loss: 0.4503, valid_loss: 0.4518, valid_accuracy: 0.8332\n",
      "Time taken for epoch 2: 0:02:21.543747\n",
      "Epoch [3], train_loss: 0.4265, valid_loss: 0.4458, valid_accuracy: 0.8362\n",
      "Time taken for epoch 3: 0:02:21.584321\n",
      "Epoch [4], train_loss: 0.4048, valid_loss: 0.4327, valid_accuracy: 0.8444\n",
      "Time taken for epoch 4: 0:02:21.979123\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "history = model.fit(model, train_batches, valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d7e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
